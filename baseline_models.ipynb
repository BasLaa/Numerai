{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "024441d4-c165-4d24-8b94-e5183d505118",
   "metadata": {
    "collapsed": true,
    "gradient": {
     "editing": false,
     "id": "024441d4-c165-4d24-8b94-e5183d505118",
     "kernelId": "d7100d46-c71c-4527-bb2f-55e2bdb581c1"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\n",
      "Requirement already satisfied: halo in /opt/conda/lib/python3.9/site-packages (0.0.31)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.9/site-packages (7.0.0)\n",
      "Requirement already satisfied: numerapi in /opt/conda/lib/python3.9/site-packages (2.9.4)\n",
      "Requirement already satisfied: lightgbm in /opt/conda/lib/python3.9/site-packages (3.3.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from halo) (1.16.0)\n",
      "Requirement already satisfied: spinners>=0.0.24 in /opt/conda/lib/python3.9/site-packages (from halo) (0.0.24)\n",
      "Requirement already satisfied: colorama>=0.3.9 in /opt/conda/lib/python3.9/site-packages (from halo) (0.4.4)\n",
      "Requirement already satisfied: log-symbols>=0.0.14 in /opt/conda/lib/python3.9/site-packages (from halo) (0.0.14)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from halo) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from pyarrow) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.9/site-packages (from numerapi) (2.8.2)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from numerapi) (1.4.1)\n",
      "Requirement already satisfied: tqdm>=4.29.1 in /opt/conda/lib/python3.9/site-packages (from numerapi) (4.63.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.9/site-packages (from numerapi) (2021.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from numerapi) (2.27.1)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from numerapi) (8.0.4)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.0.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->numerapi) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->numerapi) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->numerapi) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->numerapi) (2021.10.8)\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "477"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "!pip install halo pyarrow numerapi lightgbm\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from halo import Halo\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numerapi\n",
    "import lightgbm\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "sys.path.insert(0, './utils')\n",
    "\n",
    "from utils import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    neutralize,\n",
    "    validation_metrics,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    "    EXAMPLE_PREDS_COL\n",
    ")\n",
    "from metrics import evaluate\n",
    "from cross_validation import PurgedTimeSeriesSplitGroups\n",
    "\n",
    "from numerapi import NumerAPI\n",
    "napi = NumerAPI()\n",
    "\n",
    "current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament\n",
    "\n",
    "train_pq_path = \"./numeraidata/numerai_training_data.parquet\"\n",
    "\n",
    "# read in just those features along with era and target columns\n",
    "with open(\"./numeraidata/features.json\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "\n",
    "features_small = set(feature_metadata[\"feature_sets\"][\"small\"])\n",
    "features_medium = set(feature_metadata[\"feature_sets\"][\"medium\"])\n",
    "\n",
    "in_medium_not_in_small = features_medium - features_small\n",
    "features = list(features_small) + list(in_medium_not_in_small)\n",
    "\n",
    "read_columns = features + [DATA_TYPE_COL, ERA_COL, TARGET_COL]\n",
    "df_train = pd.read_parquet(train_pq_path, columns=read_columns)\n",
    "\n",
    "eras = df_train.era.astype(int)\n",
    "df_train[\"era\"] = eras\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a67cf7c-14cd-4af9-be95-3350742647eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def scoring_func(df, pred_col=\"prediction\"):\n",
    "\n",
    "  def _getcorrs(sub_df):\n",
    "    ranked_preds = sub_df[pred_col].rank(pct=True, method=\"first\")\n",
    "    return np.corrcoef(ranked_preds, sub_df[TARGET_COL])[0, 1]\n",
    "  \n",
    "  corrs = df.groupby(\"era\").apply(_getcorrs)\n",
    "  mean_corr = corrs.mean()\n",
    "  sharpe_ratio = mean_corr / corrs.std()\n",
    "\n",
    "  return mean_corr, sharpe_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49ce0f09-b35d-4f06-b363-02ccbcbe5acf",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "49ce0f09-b35d-4f06-b363-02ccbcbe5acf",
     "kernelId": "d7100d46-c71c-4527-bb2f-55e2bdb581c1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entering time series cross validation loop\n",
      "doing split 1 out of 3\n",
      "training models\n",
      "predicting models\n",
      "evaluating models\n",
      "scores for model trained on small:  corr: 0.05203828829452122, sharpe: 1.7218079348899469\n",
      "scores for model trained on medium:  corr: 0.05009236402949101, sharpe: 1.5819559632983888\n",
      "doing split 2 out of 3\n",
      "training models\n",
      "predicting models\n",
      "evaluating models\n",
      "scores for model trained on small:  corr: 0.05081116989184061, sharpe: 1.5833631106401396\n",
      "scores for model trained on medium:  corr: 0.054754276753451944, sharpe: 1.4379924079715303\n",
      "doing split 3 out of 3\n",
      "training models\n",
      "predicting models\n",
      "evaluating models\n",
      "scores for model trained on small:  corr: 0.0423433221748213, sharpe: 1.799960584928587\n",
      "scores for model trained on medium:  corr: 0.04859222074436574, sharpe: 2.0458342283134963\n",
      "final score for small model: corr: 0.04839759345372771, sharpe: 1.7017105434862245\n",
      "final score for medium model: corr: 0.05114628717576956, sharpe: 1.6885941998611385\n",
      "\r"
     ]
    }
   ],
   "source": [
    "base_params = {\n",
    "    'n_estimators': 2000, \n",
    "    'learning_rate': 0.01, \n",
    "    'num_leaves': 30, \n",
    "    'max_depth': 5, \n",
    "    'colsample_bytree': 0.1,\n",
    "    }\n",
    "    \n",
    "\n",
    "n_splits = 3\n",
    "purge = 5\n",
    "split = 0\n",
    "\n",
    "cv=PurgedTimeSeriesSplitGroups(n_splits, purge)\n",
    "\n",
    "corrs_s = []\n",
    "sharpes_s = []\n",
    "corrs_m = []\n",
    "sharpes_m = []\n",
    "\n",
    "# get out of sample training preds via embargoed time series cross validation\n",
    "print(f\"entering time series cross validation loop\")\n",
    "for train_index, test_index in (cv.split(df_train[features], df_train[TARGET_COL], eras)):\n",
    "    gc.collect()\n",
    "    print(f\"doing split {split+1} out of {n_splits}\")\n",
    "    \n",
    "    train_eras = df_train.iloc[train_index, :].era.unique()\n",
    "    test_eras = df_train.iloc[test_index, :].era.unique()\n",
    "    train_split_index = df_train[ERA_COL].isin(train_eras)\n",
    "    test_split_index = df_train[ERA_COL].isin(test_eras)\n",
    "\n",
    "            \n",
    "    print(f'training models')\n",
    "    model_small = lightgbm.LGBMRegressor(**base_params)\n",
    "    model_small.fit(df_train.loc[train_split_index, features_small],\n",
    "              df_train.loc[train_split_index, [TARGET_COL]])\n",
    "    \n",
    "    model_medium = lightgbm.LGBMRegressor(**base_params)\n",
    "    model_medium.fit(df_train.loc[train_split_index, features_medium],\n",
    "              df_train.loc[train_split_index, [TARGET_COL]])\n",
    "          \n",
    "    print(\"predicting models\")\n",
    "    df_train.loc[test_split_index, \"prediction_s\"] = model_small.predict(df_train.loc[test_split_index, features_small])\n",
    "    df_train.loc[test_split_index, \"prediction_m\"] = model_medium.predict(df_train.loc[test_split_index, features_medium])\n",
    "\n",
    "    print(f\"evaluating models\")\n",
    "    corr_s, sharpe_s = scoring_func(df_train.loc[test_split_index], pred_col=\"prediction_s\")\n",
    "    print(f'scores for model trained on small:  corr: {corr_s}, sharpe: {sharpe_s}')\n",
    "    \n",
    "    corr_m, sharpe_m = scoring_func(df_train.loc[test_split_index], pred_col=\"prediction_m\")\n",
    "    print(f'scores for model trained on medium:  corr: {corr_m}, sharpe: {sharpe_m}')\n",
    "    \n",
    "    corrs_s.append(corr_s)\n",
    "    corrs_m.append(corr_m)\n",
    "          \n",
    "    sharpes_s.append(sharpe_s)\n",
    "    sharpes_m.append(sharpe_m)\n",
    "    \n",
    "    split += 1\n",
    "\n",
    "print(f\"final score for small model: corr: {np.mean(corrs_s)}, sharpe: {np.mean(sharpes_s)}\")\n",
    "print(f\"final score for medium model: corr: {np.mean(corrs_m)}, sharpe: {np.mean(sharpes_m)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5176d671-0044-40f8-bda3-c54edb406ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading minimal features of validation data...\n",
      "predicting models on validation data\n",
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"doing a full train of the models\")\n",
    "model_small = lightgbm.LGBMRegressor(**base_params)\n",
    "model_small.fit(df_train.loc[:, features_small],\n",
    "          df_train.loc[:, [TARGET_COL]])\n",
    "\n",
    "model_medium = lightgbm.LGBMRegressor(**base_params)\n",
    "model_medium.fit(df_train.loc[:, features_medium],\n",
    "          df_train.loc[:, [TARGET_COL]])\n",
    "\n",
    "# getting validation data\n",
    "print('Reading minimal features of validation data...')\n",
    "validation_data = pd.read_parquet('numeraidata/numerai_validation_data.parquet',\n",
    "                                  columns=read_columns)\n",
    "    \n",
    "print(\"predicting models on validation data\")\n",
    "validation_data.loc[:, f\"prediction_s\"] = model_small.predict(validation_data.loc[:, features_small])\n",
    "validation_data.loc[:, f\"prediction_m\"] = model_medium.predict(validation_data.loc[:, features_medium])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd3cd35c-80e5-44b3-9a95-a0dfc51f7337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "with open(\"models/lgb2000_0.01_small.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_small, f)\n",
    "with open(\"models/lgb2000_0.01_med.pkl\", \"wb\") as g:\n",
    "    pickle.dump(model_medium, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffa28938-c20a-4ea5-955e-cca15c3fb64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute feature correlations with target\n",
      "compute feature exposures\n",
      "neutralizing\n",
      "\r"
     ]
    }
   ],
   "source": [
    "print(\"compute feature correlations with target\")\n",
    "all_feature_corrs = df_train.groupby('era').apply(lambda d: d[features].corrwith(d[TARGET_COL]))\n",
    "# compute the volatility of the feature correlations\n",
    "feature_corr_volatility = all_feature_corrs.std()\n",
    "\n",
    "print(\"compute feature exposures\")\n",
    "# calculate the feature exposures of the predictions\n",
    "feature_exposure_list = []\n",
    "for feature in features:\n",
    "    feature_exposure_list.append(np.corrcoef(df_train[feature], df_train[\"prediction_m\"])[0,1])\n",
    "feature_exposure_list = pd.Series(feature_exposure_list, index=features)\n",
    "\n",
    "# neutralize our predictions to the riskiest features\n",
    "riskiest_features = (feature_exposure_list.abs()*feature_corr_volatility).sort_values()[-100:].index.tolist()\n",
    "\n",
    "print(\"neutralizing\")\n",
    "validation_data[f\"prediction_s_neutral_riskiest_100_0.5\"] = neutralize(\n",
    "    df=validation_data,\n",
    "    columns=[f\"prediction_s\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=0.5,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n",
    "\n",
    "validation_data[f\"prediction_m_neutral_riskiest_100_0.5\"] = neutralize(\n",
    "    df=validation_data,\n",
    "    columns=[f\"prediction_m\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=0.5,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n",
    "\n",
    "validation_data[f\"prediction_s_neutral_riskiest_100_1.0\"] = neutralize(\n",
    "    df=validation_data,\n",
    "    columns=[f\"prediction_s\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n",
    "\n",
    "validation_data[f\"prediction_m_neutral_riskiest_100_1.0\"] = neutralize(\n",
    "    df=validation_data,\n",
    "    columns=[f\"prediction_m\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=1.0,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2d89caf-f685-47e0-9428-02dc8578eca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>sharpe</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>apy</th>\n",
       "      <th>mmc_mean</th>\n",
       "      <th>corr_plus_mmc_sharpe</th>\n",
       "      <th>corr_with_example_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>prediction_s</th>\n",
       "      <td>0.022195</td>\n",
       "      <td>0.031914</td>\n",
       "      <td>0.695462</td>\n",
       "      <td>-0.139106</td>\n",
       "      <td>186.253624</td>\n",
       "      <td>0.007552</td>\n",
       "      <td>0.609771</td>\n",
       "      <td>0.485103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_m</th>\n",
       "      <td>0.020781</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.659712</td>\n",
       "      <td>-0.196412</td>\n",
       "      <td>167.663651</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>0.530506</td>\n",
       "      <td>0.684220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_s_neutral_riskiest_100_0.5</th>\n",
       "      <td>0.023630</td>\n",
       "      <td>0.027827</td>\n",
       "      <td>0.849166</td>\n",
       "      <td>-0.108358</td>\n",
       "      <td>208.402721</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.770073</td>\n",
       "      <td>0.478299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_m_neutral_riskiest_100_0.5</th>\n",
       "      <td>0.023231</td>\n",
       "      <td>0.027464</td>\n",
       "      <td>0.845879</td>\n",
       "      <td>-0.106577</td>\n",
       "      <td>202.746749</td>\n",
       "      <td>0.003962</td>\n",
       "      <td>0.742243</td>\n",
       "      <td>0.711463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_s_neutral_riskiest_100_1.0</th>\n",
       "      <td>0.022794</td>\n",
       "      <td>0.022483</td>\n",
       "      <td>1.013797</td>\n",
       "      <td>-0.055818</td>\n",
       "      <td>198.179289</td>\n",
       "      <td>0.009332</td>\n",
       "      <td>0.916884</td>\n",
       "      <td>0.420382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prediction_m_neutral_riskiest_100_0.1.0</th>\n",
       "      <td>0.023177</td>\n",
       "      <td>0.020041</td>\n",
       "      <td>1.156473</td>\n",
       "      <td>-0.067451</td>\n",
       "      <td>204.457188</td>\n",
       "      <td>0.005380</td>\n",
       "      <td>1.065684</td>\n",
       "      <td>0.639433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             mean       std    sharpe  \\\n",
       "prediction_s                             0.022195  0.031914  0.695462   \n",
       "prediction_m                             0.020781  0.031500  0.659712   \n",
       "prediction_s_neutral_riskiest_100_0.5    0.023630  0.027827  0.849166   \n",
       "prediction_m_neutral_riskiest_100_0.5    0.023231  0.027464  0.845879   \n",
       "prediction_s_neutral_riskiest_100_1.0    0.022794  0.022483  1.013797   \n",
       "prediction_m_neutral_riskiest_100_0.1.0  0.023177  0.020041  1.156473   \n",
       "\n",
       "                                         max_drawdown         apy  mmc_mean  \\\n",
       "prediction_s                                -0.139106  186.253624  0.007552   \n",
       "prediction_m                                -0.196412  167.663651  0.002604   \n",
       "prediction_s_neutral_riskiest_100_0.5       -0.108358  208.402721  0.008800   \n",
       "prediction_m_neutral_riskiest_100_0.5       -0.106577  202.746749  0.003962   \n",
       "prediction_s_neutral_riskiest_100_1.0       -0.055818  198.179289  0.009332   \n",
       "prediction_m_neutral_riskiest_100_0.1.0     -0.067451  204.457188  0.005380   \n",
       "\n",
       "                                         corr_plus_mmc_sharpe  \\\n",
       "prediction_s                                         0.609771   \n",
       "prediction_m                                         0.530506   \n",
       "prediction_s_neutral_riskiest_100_0.5                0.770073   \n",
       "prediction_m_neutral_riskiest_100_0.5                0.742243   \n",
       "prediction_s_neutral_riskiest_100_1.0                0.916884   \n",
       "prediction_m_neutral_riskiest_100_0.1.0              1.065684   \n",
       "\n",
       "                                         corr_with_example_preds  \n",
       "prediction_s                                            0.485103  \n",
       "prediction_m                                            0.684220  \n",
       "prediction_s_neutral_riskiest_100_0.5                   0.478299  \n",
       "prediction_m_neutral_riskiest_100_0.5                   0.711463  \n",
       "prediction_s_neutral_riskiest_100_1.0                   0.420382  \n",
       "prediction_m_neutral_riskiest_100_0.1.0                 0.639433  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "validation_preds = pd.read_parquet('numeraidata/example_validation_predictions.parquet')\n",
    "validation_data[EXAMPLE_PREDS_COL] = validation_preds[\"prediction\"]\n",
    "\n",
    "validation_stats = validation_metrics(validation_data, [\"prediction_s\", \"prediction_m\", \"prediction_s_neutral_riskiest_100_0.5\", \"prediction_m_neutral_riskiest_100_0.5\", \"prediction_s_neutral_riskiest_100_1.0\", \"prediction_m_neutral_riskiest_100_0.1.0\"], example_col=EXAMPLE_PREDS_COL, fast_mode=True)\n",
    "validation_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98056c7-9fa6-41ce-996a-f97322ead77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
