{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4e703-2848-43eb-b643-6f9a703b069d",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 1,
     "id": "ebd4e703-2848-43eb-b643-6f9a703b069d",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\n",
      "Collecting halo\n",
      "  Downloading halo-0.0.31.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pyarrow\n",
      "  Downloading pyarrow-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.6 MB)\n",
      "     |████████████████████████████████| 25.6 MB 1.6 MB/s             \n",
      "\u001b[?25hCollecting numerapi\n",
      "  Downloading numerapi-2.9.4-py3-none-any.whl (26 kB)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "     |████████████████████████████████| 2.0 MB 7.1 MB/s            \n",
      "\u001b[?25hCollecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting optuna\n",
      "  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n",
      "     |████████████████████████████████| 308 kB 26.4 MB/s            \n",
      "\u001b[?25hCollecting log_symbols>=0.0.14\n",
      "  Downloading log_symbols-0.0.14-py3-none-any.whl (3.1 kB)\n",
      "Collecting spinners>=0.0.24\n",
      "  Downloading spinners-0.0.24-py3-none-any.whl (5.5 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: colorama>=0.3.9 in /opt/conda/lib/python3.9/site-packages (from halo) (0.4.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.9/site-packages (from halo) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.9/site-packages (from pyarrow) (1.20.3)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.9/site-packages (from numerapi) (8.0.3)\n",
      "Requirement already satisfied: tqdm>=4.29.1 in /opt/conda/lib/python3.9/site-packages (from numerapi) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from numerapi) (2.27.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.9/site-packages (from numerapi) (2021.3)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from numerapi) (1.3.5)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.9/site-packages (from numerapi) (2.8.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.9/site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /opt/conda/lib/python3.9/site-packages (from lightgbm) (1.0.2)\n",
      "Collecting cliff\n",
      "  Downloading cliff-3.10.0-py3-none-any.whl (80 kB)\n",
      "     |████████████████████████████████| 80 kB 19.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.9/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.9/site-packages (from optuna) (1.4.29)\n",
      "Requirement already satisfied: alembic in /opt/conda/lib/python3.9/site-packages (from optuna) (1.7.5)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from optuna) (21.3)\n",
      "Collecting cmaes>=0.8.2\n",
      "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->optuna) (3.0.6)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.0.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.9/site-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.9/site-packages (from alembic->optuna) (1.1.6)\n",
      "Collecting cmd2>=1.0.0\n",
      "  Downloading cmd2-2.3.3-py3-none-any.whl (149 kB)\n",
      "     |████████████████████████████████| 149 kB 25.8 MB/s            \n",
      "\u001b[?25hCollecting autopage>=0.4.0\n",
      "  Downloading autopage-0.4.0-py3-none-any.whl (20 kB)\n",
      "Collecting PrettyTable>=0.7.2\n",
      "  Downloading prettytable-3.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting stevedore>=2.0.1\n",
      "  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
      "     |████████████████████████████████| 49 kB 29.1 MB/s            \n",
      "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.8.0-py2.py3-none-any.whl (112 kB)\n",
      "     |████████████████████████████████| 112 kB 6.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->numerapi) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->numerapi) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->numerapi) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->numerapi) (3.1)\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.9/site-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
      "Collecting pyperclip>=1.6\n",
      "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.9/site-packages (from Mako->alembic->optuna) (2.0.1)\n",
      "Building wheels for collected packages: halo, termcolor, pyperclip\n",
      "  Building wheel for halo (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for halo: filename=halo-0.0.31-py3-none-any.whl size=11261 sha256=98887aed43c84a20a375c812314a6fc2ad082078d2f2d49817c6f72cd730311c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-02xg3rdn/wheels/bb/85/47/b7c7338ab52808105f937bd8c04aec5d98a543311ac2c8bed2\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=548da0d4c645e1665ea87a548c29003466229827b6ac979eceaaaa6260eb6360\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-02xg3rdn/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=c2f6f31689deac0e125989f2228c93655a331cb9356323890648b7f642cf5f22\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-02xg3rdn/wheels/0c/09/9e/49e21a6840ef7955b06d47394afef0058f0378c0914e48b8b8\n",
      "Successfully built halo termcolor pyperclip\n",
      "Installing collected packages: pyperclip, pbr, stevedore, PrettyTable, cmd2, autopage, termcolor, spinners, log-symbols, colorlog, cmaes, cliff, tabulate, pyarrow, optuna, numerapi, lightgbm, halo\n",
      "Successfully installed PrettyTable-3.0.0 autopage-0.4.0 cliff-3.10.0 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 halo-0.0.31 lightgbm-3.3.2 log-symbols-0.0.14 numerapi-2.9.4 optuna-2.10.0 pbr-5.8.0 pyarrow-6.0.1 pyperclip-1.8.2 spinners-0.0.24 stevedore-3.5.0 tabulate-0.8.9 termcolor-1.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Downloading dataset files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-16 20:49:23,234 INFO numerapi.utils: target file already exists\n",
      "2022-01-16 20:49:23,235 INFO numerapi.utils: download complete\n",
      "2022-01-16 20:49:23,778 INFO numerapi.utils: target file already exists\n",
      "2022-01-16 20:49:23,779 INFO numerapi.utils: download complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "!pip install halo pyarrow numerapi lightgbm tabulate optuna\n",
    "\n",
    "import os\n",
    "from halo import Halo\n",
    "import json\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import numerapi\n",
    "import lightgbm\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from utils import (\n",
    "    save_model,\n",
    "    load_model,\n",
    "    neutralize,\n",
    "    get_biggest_change_features,\n",
    "    get_time_series_cross_val_splits,\n",
    "    validation_metrics,\n",
    "    ERA_COL,\n",
    "    DATA_TYPE_COL,\n",
    "    TARGET_COL,\n",
    "    EXAMPLE_PREDS_COL\n",
    ")\n",
    "\n",
    "from metrics import evaluate\n",
    "\n",
    "from numerapi import NumerAPI\n",
    "napi = NumerAPI()\n",
    "\n",
    "current_round = napi.get_current_round(tournament=8)  # tournament 8 is the primary Numerai Tournament\n",
    "\n",
    "# Tournament data changes every week so we specify the round in their name. Training\n",
    "# and validation data only change periodically, so no need to download them every time.\n",
    "print('Downloading dataset files...')\n",
    "napi.download_dataset(\"numerai_training_data_int8.parquet\", \"training_data_int8.parquet\")\n",
    "# napi.download_dataset(\"numerai_tournament_data_int8.parquet\", f\"tournament_data_int8_{current_round}.parquet\")\n",
    "# napi.download_dataset(\"numerai_validation_data_int8.parquet\", f\"validation_data_int8.parquet\")\n",
    "# napi.download_dataset(\"example_validation_predictions.parquet\", \"example_validation_predictions.parquet\")\n",
    "napi.download_dataset(\"features.json\", \"features.json\")\n",
    "\n",
    "with open(\"features.json\", \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "features_small = set(feature_metadata[\"feature_sets\"][\"small\"])\n",
    "features_medium = set(feature_metadata[\"feature_sets\"][\"medium\"])\n",
    "\n",
    "in_medium_not_in_small = features_medium - features_small\n",
    "features = list(features_small) + list(in_medium_not_in_small)\n",
    "\n",
    "train_pq_path = \"training_data_int8.parquet\"\n",
    "\n",
    "# read in just those features along with era and target columns\n",
    "read_columns = features + [ERA_COL, DATA_TYPE_COL, TARGET_COL]\n",
    "df_train = pd.read_parquet(train_pq_path, columns=read_columns)\n",
    "\n",
    "eras = df_train.era.astype(int)\n",
    "df_train[\"era\"] = eras\n",
    "\n",
    "training_data = df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dfd7f0-f317-4861-8afe-507ffa49c87d",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 2,
     "id": "83dfd7f0-f317-4861-8afe-507ffa49c87d",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "def era_boosting_train(model, X, y, era_col, proportion, ne, ni):\n",
    "    features = X.columns\n",
    "    model.fit(X, y)\n",
    "    new_df = X.copy()\n",
    "    new_df[TARGET_COL] = y\n",
    "    new_df[\"era\"] = era_col\n",
    "\n",
    "    for i in range(ni-1):\n",
    "        preds = model.predict(X)\n",
    "        new_df[\"pred\"] = preds\n",
    "        era_scores = pd.Series(dtype='float32', index=new_df[\"era\"].unique())\n",
    "        for era in new_df[\"era\"].unique():\n",
    "            era_df = new_df[new_df[\"era\"] == era]\n",
    "            era_scores[era] = spearmanr(era_df[\"pred\"], era_df[TARGET_COL])[0]\n",
    "        \n",
    "        era_scores.sort_values(inplace=False)\n",
    "        worst_eras = era_scores[era_scores <= era_scores.quantile(proportion)].index\n",
    "\n",
    "        worst_df = new_df[new_df[\"era\"].isin(worst_eras)]\n",
    "        era_scores.sort_index(inplace=True)\n",
    "        \n",
    "        print(f\"ne: {model.n_estimators}, iter: {i+1}, mean corr: {np.mean(era_scores)}, sharpe: {np.mean(era_scores)/np.std(era_scores)}\")\n",
    "        \n",
    "        model.fit(worst_df[features], worst_df[TARGET_COL], init_model = model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e5bc5-5733-40f2-b298-fe92574c3478",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 5,
     "id": "de0e5bc5-5733-40f2-b298-fe92574c3478",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ne: 250, iter: 1, mean corr: 0.06882654875516891, sharpe: 2.3689191621029577\n",
      "ne: 250, iter: 2, mean corr: 0.07703043520450592, sharpe: 4.084805086898683\n",
      "ne: 250, iter: 3, mean corr: 0.0848427340388298, sharpe: 5.144846619523415\n",
      "ne: 250, iter: 4, mean corr: 0.08879073709249496, sharpe: 6.47585416343457\n",
      "ne: 250, iter: 5, mean corr: 0.09365268051624298, sharpe: 6.301480959693916\n",
      "ne: 250, iter: 6, mean corr: 0.09600390493869781, sharpe: 7.923205554730569\n",
      "ne: 250, iter: 7, mean corr: 0.10016566514968872, sharpe: 6.990891605299559\n",
      "ne: 250, iter: 8, mean corr: 0.10243947803974152, sharpe: 9.374425154250295\n",
      "\r"
     ]
    }
   ],
   "source": [
    "# For era boosting, the total number of estimators built will be ne*ni\n",
    "\n",
    "ni = 9\n",
    "ne = 250\n",
    "proportion = 0.5\n",
    "param_grid = {\n",
    "    'n_estimators': ne, \n",
    "    'learning_rate': 0.003, \n",
    "    'num_leaves': 25, \n",
    "    'max_depth': 6, \n",
    "    'colsample_bytree': 0.1,\n",
    "    \"max_bin\": 50,\n",
    "}\n",
    "\n",
    "temp_model = lightgbm.LGBMRegressor(**param_grid)\n",
    "model = era_boosting_train(\n",
    "    temp_model,\n",
    "    training_data[features],\n",
    "    training_data[TARGET_COL],\n",
    "    era_col=training_data['era'],\n",
    "    proportion=proportion, \n",
    "    ne=ne,\n",
    "    ni=ni\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d6ea3-b792-4cea-800a-75289dec0b34",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 9,
     "id": "5f6d6ea3-b792-4cea-800a-75289dec0b34",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading minimal features of validation and tournament data...\n",
      "predicting on validation data\n",
      "\r"
     ]
    }
   ],
   "source": [
    "print('Reading minimal features of validation and tournament data...')\n",
    "validation_data = pd.read_parquet('validation_data_int8.parquet',\n",
    "                                  columns=read_columns)\n",
    "\n",
    "print(\"predicting on validation data\")\n",
    "validation_data.loc[:, \"prediction\"] = model.predict(\n",
    "    validation_data.loc[:, features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74307539-e9d9-4d0e-bb7e-8aa2c5bb8151",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 8,
     "id": "74307539-e9d9-4d0e-bb7e-8aa2c5bb8151",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# compute feature correlations with target\n",
    "all_feature_corrs = training_data.groupby('era').apply(lambda d: d[features].corrwith(d[TARGET_COL]))\n",
    "# compute the volatility of the feature correlations\n",
    "feature_corr_volatility = all_feature_corrs.std()\n",
    "\n",
    "# calculate the feature exposures of the predictions\n",
    "feature_exposure_list_val = []\n",
    "for feature in features:\n",
    "    feature_exposure_list_val.append(np.corrcoef(validation_data[feature], validation_data[\"prediction\"])[0,1])\n",
    "feature_exposure_list_val = pd.Series(feature_exposure_list_val, index=features)\n",
    "\n",
    "riskiest_features = (feature_exposure_list_val.abs()*feature_corr_volatility).sort_values()[-100:].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b779a0-311b-4921-bc49-8f9ebc346175",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 13,
     "id": "96b779a0-311b-4921-bc49-8f9ebc346175",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5022857243144736"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "feature_exposure_list_val.abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3be6d3-1ba7-498d-b50f-e09ece77b7ee",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "execution_count": 18,
     "id": "4d3be6d3-1ba7-498d-b50f-e09ece77b7ee",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# neutralize our predictions to the riskiest features\n",
    "validation_data[f\"preds_neutral_riskiest_50\"] = neutralize(\n",
    "    df=validation_data,\n",
    "    columns=[f\"prediction\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=0.75,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403ef3d-62e1-47a6-85a3-da28d0114800",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 19,
     "id": "1403ef3d-62e1-47a6-85a3-da28d0114800",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for prediction\n",
      "Spearman Correlation: 0.021\n",
      "Sharpe Ratio: 0.664\n",
      "Max Feature Exposure:  0.513\n",
      "Metrics for preds_neutral_riskiest_50\n",
      "Spearman Correlation: 0.0224\n",
      "Sharpe Ratio: 0.942\n",
      "Max Feature Exposure:  0.1959\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>preds_neutral_riskiest_50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spearman</th>\n",
       "      <td>0.021</td>\n",
       "      <td>0.0224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharpe</th>\n",
       "      <td>0.664</td>\n",
       "      <td>0.9420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_feature_exposure</th>\n",
       "      <td>0.513</td>\n",
       "      <td>0.1959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      prediction  preds_neutral_riskiest_50\n",
       "spearman                   0.021                     0.0224\n",
       "sharpe                     0.664                     0.9420\n",
       "max_feature_exposure       0.513                     0.1959"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "evaluate(validation_data, [\"prediction\", \"preds_neutral_riskiest_50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfa0e7-fbfc-459e-8f6f-1a298782eaf0",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": true,
     "execution_count": 21,
     "id": "94bfa0e7-fbfc-459e-8f6f-1a298782eaf0",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on tournament data\n",
      "\r"
     ]
    }
   ],
   "source": [
    "print('Reading minimal features of tournament data...')\n",
    "napi.download_dataset(\"numerai_tournament_data.parquet\", f\"tournament_data_{current_round}.parquet\")\n",
    "\n",
    "tournament_data = pd.read_parquet(f'tournament_data_{current_round}.parquet',\n",
    "                                  columns=read_columns)\n",
    "\n",
    "print(\"predicting on tournament data\")\n",
    "tournament_data.loc[:, f\"prediction\"] = model.predict(\n",
    "    tournament_data.loc[:, features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da890d1-3746-4371-9a4a-7f856e2957d7",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 22,
     "id": "7da890d1-3746-4371-9a4a-7f856e2957d7",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# calculate the feature exposures of the predictions\n",
    "feature_exposure_list_tourny = []\n",
    "for feature in features:\n",
    "    feature_exposure_list_tourny.append(np.corrcoef(tournament_data[feature], tournament_data[\"prediction\"])[0,1])\n",
    "feature_exposure_list_tourny = pd.Series(feature_exposure_list_tourny, index=features)\n",
    "\n",
    "riskiest_features = (feature_exposure_list_tourny.abs()*feature_corr_volatility).sort_values()[-100:].index.tolist()\n",
    "\n",
    "# neutralize our predictions to the riskiest features\n",
    "tournament_data[f\"preds_neutral_riskiest_100\"] = neutralize(\n",
    "    df=tournament_data,\n",
    "    columns=[f\"prediction\"],\n",
    "    neutralizers=riskiest_features,\n",
    "    proportion=0.75,\n",
    "    normalize=True,\n",
    "    era_col=ERA_COL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f562775-37cc-47cc-982d-0e2425f8393c",
   "metadata": {
    "gradient": {
     "editing": false,
     "execution_count": 23,
     "id": "3f562775-37cc-47cc-982d-0e2425f8393c",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "tournament_data[\"prediction\"] = tournament_data[\"preds_neutral_riskiest_100\"].rank(pct=True)\n",
    "tournament_data[\"prediction\"].to_csv(f\"tournament_predictions_{current_round}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeb8758-d2d2-4ea5-ad99-3b6dc79e7f57",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "aeeb8758-d2d2-4ea5-ad99-3b6dc79e7f57",
     "kernelId": "fc3a7656-1b03-4e13-8f22-14d1f75d0ad6"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
